**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 – Software Reliability Assessment**

| Group \#:       |   |
|-----------------|---|
| Student Names:  | Shahzill Naveed  |
|                 | Muteeba Jamal  |
|                 | Rumaisa Talukder  |
|                 | Iman Niaz  |

# Introduction
This lab includes analysis of integration test data using reliability assessment tools. We explored two ways to assess failure data:
1. Reliability growth testing: SRTAT
2. Reliability assessment using Reliability Demonstration Chart (RDC): Reliability-Demonstration-Chart.xls

By using the SRTAT tool we will: 
* gain an understanding of what reliability growth testing is and why it is useful.
* be able to measure the failure rate, MTTF and reliability of the SUT through analyzing the test data.
* become familiar with the features and usage of a reliability growth testing tool.

After using the reliability demonstration chart excel we will: 
* be able to decide upon adequacy of testing for a given MTTF of the SUT through plotting thetest data.
* become familiar with the features and usage of an RDCtool.

# 

# Assessment Using Reliability Growth Testing 

## Failure vs Time

![Failure vs time](https://user-images.githubusercontent.com/110129714/228982104-76de21eb-b8bd-42a8-a8f3-03c7172533d8.PNG)

## Failure Intensity

![FailureIntensity](https://user-images.githubusercontent.com/110129714/228982211-2bd50452-1fa0-4c66-95b2-27fcafb75c39.PNG)

## Best Fit

![BestFit](https://user-images.githubusercontent.com/110129714/228982220-aa572cb3-e587-4509-9fda-bc2b88b39022.PNG)

We used C-SFRAT to do our reliability growth testing since SRTAT would not work properly on our machines. We used the JD4 dataset for our testing and found that DW3 and DW2 were best fit models for that dataset. The failure intensity peaks during the 39th errror So the acceptable range of failure rate for the dataset is up till the 39th error after that we cut off the data to avoid data inconsistency. A relability graph was not generated for this dataset since we did not have enough data. 

## Advantages and Disadvantages
### Advantages
* Reduces the need of costly repairs by dentifying defects and errors earlier on
* Helps prevent product failures and improve product reliability

### Disadvantages
* Time-consuming process, which may lead to delay in product launch
* Only able to detect defects that occur during the testing phase, limiting its effectiveness


# Assessment Using Reliability Demonstration Chart 
For the Reliability Demonstration Charts we chose the following parameters:
* Discrimination Ratio, γ:	2.000
* Developer's Risk, α:	0.100
* User's Risk, β:	0.100

We used the failure count data in J1.DAT to plot our charts. 

## Evaluation and justification for MTTFmin
To determine the appropriate value of MTTFmin, we adjusted the values of 'Maximum Acceptable Number of Failures' and 'Per Number of input events' until the reliability plot showed that the probability of failure fell within the acceptance region. When there was at least one plot in the accept region, although unreliable, we figured this would be the minimum MTTF for which the system could possibly be accepted.

## MTTFmin 
The following values helped us get the MTTFmin:
* Maximum Acceptable Number of Failures: 17
* Per Number of input events: 10 weeks

You can see in the figure below that the last point is plotted in the accept region. Althought the SUT is in the accept region, developers may not accept the system since it isn't very reliable. 
![MTTFmin](/media/MTTFmin.jpg)

## Double MTTFmin 
We doubled the number of weeks to increase the FIO value by two times.

The following values helped us get the DoubleMTTFmin:
* Maximum Acceptable Number of Failures: 17
* Per Number of input events: 20 weeks

You can see in the figure below that this brought the SUT completely into the reject region. The system failed the certification test, therefore, the developers would reject the system.
![twiceMTTFmin](/media/twiceMTTFmin.jpg)

## Half MTTFmin 
We then halfed the number of weeks used in the MTTFmin calculation and plotted the RDC below. 

The following values helped us get the HalfMTTFmin:
* Maximum Acceptable Number of Failures: 17
* Per Number of input events: 5 weeks

You can see that halfing the MTTFmin brought the SUT well into the accept region, meaning the developers can accept the system with reasonable reliability.
![halfMTTFmin](/media/halfMTTFmin.jpg)

## Advantages and Disadvatages
### Advantages
* Quick and efficient if there is limited failure data
* Easy to find out the trend for reliability of the system

### Disadvantages
* Difficult to analyse larger data sets
* There is a risk of wrongly accepting or rejecting a system if parameters are not accurately chosen
* Initially difficult and time consuming to navigate the excel sheet

# 

# Comparison of Results
We used C-SFRAT for the RGT and the RDC.xls for the RDC to assess reliability. 

With the RDC.xls we saw that the SUT was in the acceptable region when we set the FIO to 17 failures per 10 weeks. This value was obtained through exploratory methods, adujusting the FIO until our SUT was just about in the accept range, giving us our MTTFMin of 0.588, as seen in the charts above. Further failures would cause the system to be in the Continue region, and further testing would be required in order for the system to be acceptable.

As for the for the RGT results we obtained them by using C-SFRAT, we used dataset JD4 to focus on time-between-failures, failure intensity and reliability graphs. We used laplace tesitng to find useful data and decided to cut off the data at the 39th error. Using this we found out that DW3 and DW2 were best fit models for that dataset.

# Discussion on Similarity and Differences of the Two Techniques
Reliability Growth Testing and Reliability Demonstration Chart are techniques used to assess the reliability of a product or a system. Both methods use statistical techniques to analyze the reliability data. Both RGT and RDC search for any improvements to be made with the project at hand to increase its reliability, as well as monitor the reliability data to identify trends and patterns to help make decisions about the product or system.

Reliability Growth Testing identifies and fixes reliability issues during the development phase of a product or system, whereas the Reliability Demonstration Chart demonstrates whether the product or system meets the reliability requirements or specifications. Unlike RGT, RDC is typically performed after the development phase as a final test before the product or system is made public. For RGT, tests are conducted under various conditions and are repeated until reliability objectives are met. For RDC, tests are conducted under specific conditions for a specific duration. 

# How the team work/effort was divided and managed
We divided the work equally among the team with one pair working on the Reliability Growth Testing, and the other pair working on the Reliability Demonstration Chart. We then came together as a group to inform eachother of our results and what we learnt. After discussing the results as a group, we got started on the report.    

# 

# Difficulties encountered, challenges overcome, and lessons learned
Although the concepts were not extremely difficult, we faced a challenge in utilising the tools. For instance, SRTAT proved to be difficult to use as it would not work on our machines.

# Comments/feedback on the lab itself
We found the instructions in the lab slightly unclear. It would have helped if we were given clearer instructions on how to use the tools required.  

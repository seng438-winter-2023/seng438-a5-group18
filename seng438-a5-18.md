**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 – Software Reliability Assessment**

| Group \#:       |   |
|-----------------|---|
| Student Names:  | Shahzill Naveed  |
|                 | Muteeba Jamal  |
|                 | Rumaisa Talukder  |
|                 | Iman Niaz  |

# Introduction

# 

# Assessment Using Reliability Growth Testing 

# Assessment Using Reliability Demonstration Chart 
For the Reliability Demonstration Charts we chose the following parameters:
* Discrimination Ratio, γ:	2.000
* Developer's Risk, α:	0.100
* User's Risk, β:	0.100

## MTTFmin 
We experimented with different values of 'Maximum Acceptable Number of Failures' and 'Per Number of input events', until we got the minimum MTTFmin for which the SUT became acceptable. 

The following values helped us get the MTTFmin:
* Maximum Acceptable Number of Failures: 17
* Per Number of input events: 10 weeks

You can see in the figure below that the last point is plotted in the accept region. Althought the SUT is in the accept region, developers may not accept the system since it isn't very reliable. 
![MTTFmin](/media/MTTFmin.jpg)

## Double MTTFmin 
We doubled the number of weeks to increase the FIO value by two times.

The following values helped us get the DoubleMTTFmin:
* Maximum Acceptable Number of Failures: 17
* Per Number of input events: 20 weeks

You can see in the figure below that this brought the SUT completely into the reject region. The system failed the certification test, therefore, the developers would reject the system.
![twiceMTTFmin](/media/twiceMTTFmin.jpg)

## Half MTTFmin 
We then halfed the number of weeks used in the MTTFmin calculation and plotted the RDC below. 

The following values helped us get the HalfMTTFmin:
* Maximum Acceptable Number of Failures: 17
* Per Number of input events: 5 weeks

You can see that halfing the MTTFmin brought the SUT well into the accept region, meaning the developers can accept the system with reasonable reliability.
![halfMTTFmin](/media/halfMTTFmin.jpg)
# 

# Comparison of Results

# Discussion on Similarity and Differences of the Two Techniques

# How the team work/effort was divided and managed

# 

# Difficulties encountered, challenges overcome, and lessons learned

# Comments/feedback on the lab itself
